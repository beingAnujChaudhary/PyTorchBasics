{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmlgm1M838Qh57vIJK3XSH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìò Introduction to PyTorch\n","\n","PyTorch is an **open-source deep learning framework** developed by **Meta AI (FAIR)**.\n","\n","It combines:\n","- Python‚Äôs simplicity\n","- Torch‚Äôs high-performance tensor engine (GPU support)\n","\n","### Why PyTorch?\n","Older frameworks were static, hard to debug, and non-Pythonic.\n","PyTorch introduced **dynamic computation graphs**, making models behave like normal Python code.\n"],"metadata":{"id":"vc-0_BicedMM"}},{"cell_type":"code","source":["import torch\n","\n","# Create a tensor\n","x = torch.tensor([1.0, 2.0, 3.0])\n","print(x)\n","print(type(x))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBEezU5FehMR","executionInfo":{"status":"ok","timestamp":1766380428219,"user_tz":-330,"elapsed":4889,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"7be5e69e-6c4e-4012-c052-e3bb02465438"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.])\n","<class 'torch.Tensor'>\n"]}]},{"cell_type":"markdown","source":["# Core Features of PyTorch"],"metadata":{"id":"zZAGSiUYfPDG"}},{"cell_type":"markdown","source":["## 1Ô∏è‚É£ Tensor Computation\n","\n","- Core data structure: `torch.Tensor`\n","- Similar to NumPy arrays\n","- Supports GPU acceleration and gradients\n"],"metadata":{"id":"f4us0j_JekFZ"}},{"cell_type":"code","source":["a = torch.tensor([1.0, 2.0])\n","b = torch.tensor([3.0, 4.0])\n","\n","# Basic operations\n","print(a + b)\n","print(a * b)\n","print(torch.dot(a, b))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdhurdCxemtn","executionInfo":{"status":"ok","timestamp":1766380434296,"user_tz":-330,"elapsed":32,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"35fbc035-4921-4be8-c957-d8ff4642f2a1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([4., 6.])\n","tensor([3., 8.])\n","tensor(11.)\n"]}]},{"cell_type":"markdown","source":["## 2Ô∏è‚É£ GPU Acceleration\n","\n","PyTorch allows seamless movement between CPU and GPU.\n"],"metadata":{"id":"vbybHbPdep21"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device)\n","\n","x = torch.randn(3, 3).to(device)\n","print(x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwfZG-THesVa","executionInfo":{"status":"ok","timestamp":1766380456511,"user_tz":-330,"elapsed":65,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"a83dd35f-fe76-4258-97fb-ad5a3c3c8e27"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","tensor([[ 0.8156, -0.9392, -0.2433],\n","        [-0.7462, -0.8970, -0.1343],\n","        [ 1.4309,  2.1398,  1.1661]])\n"]}]},{"cell_type":"markdown","source":["## 3Ô∏è‚É£ Dynamic Computation Graph\n","\n","- Graph is created **at runtime**\n","- Enables:\n","  - Conditional logic\n","  - Variable-length inputs\n","  - Easy debugging\n","\n","This is called **define-by-run**.\n"],"metadata":{"id":"GPJJQN1veuHD"}},{"cell_type":"code","source":["x = torch.tensor(2.0, requires_grad=True)\n","\n","if x > 1:\n","    y = x * 3\n","else:\n","    y = x * 2\n","\n","y.backward()\n","print(x.grad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21SuO5T0evYd","executionInfo":{"status":"ok","timestamp":1766380469061,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"a7daf752-2dad-4bd3-b89b-79aa0b2a0ed6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(3.)\n"]}]},{"cell_type":"markdown","source":["## 4Ô∏è‚É£ Automatic Differentiation (Autograd)\n","\n","PyTorch tracks operations on tensors and automatically computes gradients using backpropagation.\n"],"metadata":{"id":"l-piaIGXewtx"}},{"cell_type":"code","source":["x = torch.tensor(5.0, requires_grad=True)\n","y = x**2 + 3*x + 1\n","y.backward()\n","\n","print(\"dy/dx:\", x.grad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcmevhvRez1t","executionInfo":{"status":"ok","timestamp":1766380487196,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"51324655-47d2-428f-c013-67a7be7d0646"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["dy/dx: tensor(13.)\n"]}]},{"cell_type":"markdown","source":["## 5Ô∏è‚É£ Neural Networks with torch.nn\n","\n","`torch.nn` provides:\n","- Layers\n","- Activations\n","- Loss functions\n"],"metadata":{"id":"JO8vB6fEe1zF"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Linear(2, 4),\n","    nn.ReLU(),\n","    nn.Linear(4, 1)\n",")\n","\n","x = torch.randn(1, 2)\n","output = model(x)\n","print(output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3N4M6lvve2jG","executionInfo":{"status":"ok","timestamp":1766380504523,"user_tz":-330,"elapsed":32,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"b6aa8073-0a85-46d1-c054-5941de4b0de1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0534]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["## 6Ô∏è‚É£ Optimizers (torch.optim)\n","\n","Optimizers update model parameters using gradients.\n","Common ones:\n","- SGD\n","- Adam\n","- RMSprop\n"],"metadata":{"id":"huDDB4_ie5oo"}},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","loss_fn = nn.MSELoss()\n","target = torch.tensor([[1.0]])\n","\n","loss = loss_fn(output, target)\n","loss.backward()\n","optimizer.step()\n","\n","print(\"Loss:\", loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fGZPGoDe659","executionInfo":{"status":"ok","timestamp":1766380521458,"user_tz":-330,"elapsed":5159,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"9cd6079a-27c9-4f13-fca1-f41b9bc08149"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 1.1097168922424316\n"]}]},{"cell_type":"markdown","source":["## 7Ô∏è‚É£ Data Loading (Dataset & DataLoader)\n","\n","Efficient batching, shuffling, and loading.\n"],"metadata":{"id":"dVTKiLFte8Hp"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","X = torch.randn(100, 2)\n","y = torch.randn(100, 1)\n","\n","dataset = TensorDataset(X, y)\n","loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","for xb, yb in loader:\n","    print(xb.shape, yb.shape)\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzUANujIe9s2","executionInfo":{"status":"ok","timestamp":1766380527433,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"4aa1bb74-7d24-41b3-a292-cf429f8cb5ad"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 2]) torch.Size([16, 1])\n"]}]},{"cell_type":"markdown","source":["## 8Ô∏è‚É£ PyTorch Ecosystem\n","\n","- torchvision ‚Üí Computer Vision\n","- torchtext ‚Üí NLP\n","- torchaudio ‚Üí Audio\n","- PyTorch Lightning ‚Üí Cleaner training loops\n","- Hugging Face Transformers ‚Üí LLMs\n"],"metadata":{"id":"ae2rBeRae_DC"}},{"cell_type":"markdown","source":["##  PyTorch vs. TensorFlow\n","\n","| Aspect | PyTorch | TensorFlow | Verdict |\n","| :--- | :--- | :--- | :--- |\n","| **Language** | Pythonic interface with deep Python integration. | Supports multiple languages (C++, Java, JavaScript, Swift). | **PyTorch** (better for Python-centric development). |\n","| **Ease of Use** | Intuitive, readable syntax; easier for beginners. | Improved in 2.x with Keras, but can still feel complex. | **PyTorch** (more intuitive). |\n","| **Deployment** | Uses TorchScript and PyTorch Mobile for deployment. | Strong production tooling (TF Serving, TF Lite, TFX). | **TensorFlow** (more mature deployment ecosystem). |\n","| **Performance** | Dynamic graphs may introduce slight overhead but remain competitive. | Optimized via static graphs and XLA compiler. | **Tie** (differences often negligible in practice). |\n","| **Community** | Rapidly growing; dominant in **research**. | Large, established; dominant in **industry**. | Depends on use case. |\n","| **Customizability** | Easier to implement custom layers and operations. | Custom ops possible but often more complex. | **PyTorch**. |\n","\n","---\n","\n","## The PyTorch API Ecosystem\n","\n","### **Core Modules**\n","- `torch`: Core module for tensors and mathematical operations  \n","- `torch.autograd`: Automatic differentiation engine  \n","- `torch.nn`: Neural network layers, activations, and loss functions  \n","- `torch.optim`: Optimization algorithms (SGD, Adam, etc.)  \n","- `torch.utils.data`: `Dataset` and `DataLoader` for efficient data handling  \n","- `torch.jit`: TorchScript for compilation and production use  \n","- `torch.distributed`: Parallel and distributed computation tools  \n","- `torch.cuda`: Interface for NVIDIA GPU acceleration  \n","\n","### **Domain & Ecosystem Libraries**\n","- **Vision / Audio:** `torchvision`, `torchaudio`  \n","- **NLP:** `torchtext`, **Hugging Face Transformers** (state-of-the-art NLP & LLMs)  \n","- **Wrappers:**  \n","  - **Fastai** ‚Äì High-level API with best practices  \n","  - **PyTorch Lightning** ‚Äì Scalable training with reduced boilerplate  \n","- **Specialized:**  \n","  - **PyTorch Geometric** ‚Äì Graph Neural Networks  \n","  - **Optuna** ‚Äì Hyperparameter optimization  \n","  - **TorchServe** ‚Äì Model serving at scale  \n","\n","---\n","\n","## 6. Industry Adoption\n","\n","Major technology companies using PyTorch:\n","\n","- **Meta Platforms** ‚Äì Facebook & Instagram (computer vision, NLP)  \n","- **Tesla** ‚Äì Autopilot & Full Self-Driving (FSD)  \n","- **OpenAI** ‚Äì GPT models, DALL¬∑E, ChatGPT  \n","- **Microsoft** ‚Äì Azure Machine Learning, Bing Search  \n","- **Uber** ‚Äì Demand forecasting, routing, Pyro (probabilistic programming)  \n"],"metadata":{"id":"ojNyvLejf1Mf"}},{"cell_type":"markdown","source":["# üìÑ Summary\n","\n","## Core Concepts\n","- Tensor: GPU-enabled NumPy-like array\n","- Dynamic Graph: Built at runtime (define-by-run)\n","- Autograd: Automatic backpropagation engine\n","\n","## Key Modules\n","- torch ‚Üí tensors & math\n","- torch.nn ‚Üí layers & losses\n","- torch.optim ‚Üí optimizers\n","- torch.utils.data ‚Üí DataLoader\n","- torch.distributed ‚Üí multi-GPU training\n","- torch.jit ‚Üí TorchScript\n","- torch.onnx ‚Üí model export\n","\n","## PyTorch vs TensorFlow\n","- PyTorch ‚Üí research, LLMs, flexibility\n","- TensorFlow ‚Üí legacy production, Keras\n","\n","## Common Interview Questions\n","**Q: Why dynamic graphs?**  \n","A: Easier debugging, flexible control flow.\n","\n","**Q: What does `.backward()` do?**  \n","A: Computes gradients via autograd.\n","\n","**Q: How do you move to GPU?**  \n","A: `.to(\"cuda\")`\n","\n","**Q: What happens in training?**  \n","Forward ‚Üí loss ‚Üí backward ‚Üí optimizer.step()\n","\n","## Mental Model\n","Neural Networks = Functions on tensors + gradients\n"],"metadata":{"id":"iDUjWIVxfAVH"}}]}