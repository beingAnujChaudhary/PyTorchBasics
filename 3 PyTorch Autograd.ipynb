{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1022HrY0cW-DNMj3vG2n6-OjmRXtE8lh2","timestamp":1766549298989}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üîÑ PyTorch Autograd\n","\n","Autograd is a core component of PyTorch that provides **automatic differentiation** for tensor operations. It is the \"magic\" that calculates gradients automatically, which is essential for optimizing models (**training neural networks** ) using algorithms like Gradient Descent.\n","\n","\n","---\n","\n","## 1Ô∏è‚É£ Why Do We Need Autograd?\n","\n","Training a neural network requires:\n","- Computing **loss**\n","- Finding how loss changes w.r.t. parameters (`‚àÇL/‚àÇw`, `‚àÇL/‚àÇb`)\n","- Updating parameters using gradient descent\n","\n","Manually computing gradients for large networks is:\n","\n","‚ùå error-prone  \n","‚ùå tedious  \n","‚ùå impractical  \n","\n","‚úÖ **Autograd does this automatically**\n","\n","---\n","\n","## 2Ô∏è‚É£ Training Process (High-Level)\n","\n","Training a Neural Network involves four main steps:\n","\n","1.  **Forward Pass:** Compute the output (prediction) of the network given an input.\n","    * *Formula:* $\\hat{y} = \\sigma(w \\cdot x + b)$\n","2.  **Calculate Loss:** Measures the error by comparing the prediction $\\hat{y}$ to the actual target $y$.\n","    * *Formula (Binary Cross-Entropy):* $L = -[y \\cdot \\ln(\\hat{y}) + (1-y) \\cdot \\ln(1-\\hat{y})]$\n","3.  **Backward Pass:** Compute gradients of the loss with respect to the parameters ($\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}$) using the **Chain Rule**.\n","4.  **Update Gradients:** Adjust weights and biases to minimize error using an optimizer (e.g., Gradient Descent).\n","\n","\n","---\n","\n","### **3. The Chain Rule Explained**\n","To find how much the Loss ($L$) changes when we tweak a weight ($w$), Autograd multiplies the local derivatives \"backwards\" from the output to the input:\n","\n","$$\n","\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n","$$\n","\n","* **Step 1:** How Loss changes w.r.t Prediction ($\\frac{\\partial L}{\\partial \\hat{y}}$)\n","\n","* **Step 2:** How Prediction changes w.r.t Linear Output ($\\frac{\\partial \\hat{y}}{\\partial z}$) -> *Derivative of Sigmoid*\n","\n","* **Step 3:** How Linear Output changes w.r.t Weight ($\\frac{\\partial z}{\\partial w} = x$)\n","\n","**Final Gradient Calculation:**\n","* **For Weight:** $\\frac{\\partial L}{\\partial w} = (\\hat{y} - y) \\cdot x$\n","\n","* **For Bias:** $\\frac{\\partial L}{\\partial b} = (\\hat{y} - y) \\cdot 1$\n","\n","### **4. Example 1: Simple Function**\n","If $y = x^2$ and $x = 3$:\n","\n","1.  **Forward:** $y = 3^2 = 9$\n","\n","2.  **Backward ($\\frac{dy}{dx}$):** Derivative of $x^2$ is $2x$.\n","\n","3.  **Result:** $2(3) = 6$\n","\n","\n","### **5 Example 2: Nested Function (Chain Rule)**\n","\n","In this example, we calculate the gradient for a nested function where one operation feeds into another.\n","\n","**The Function:**\n","We have two stages:\n","1.  **Inner Function:** $y = x^2$\n","2.  **Outer Function:** $z = \\sin(y)$\n","\n","**Goal:**\n","Find the gradient of $z$ with respect to $x$ ($\\frac{dz}{dx}$).\n","\n","**Forward Pass:**\n","* Input $x$ is squared to get $y$.\n","* $y$ is passed through the sine function to get $z$.\n","\n","**Backward Pass (Chain Rule):**\n","Autograd calculates the derivative by multiplying the local gradients:\n","\n","$$\n","\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n","$$\n","\n","**Step-by-Step Calculation:**\n","1.  **Outer Derivative ($\\frac{dz}{dy}$):** The derivative of $\\sin(y)$ is $\\cos(y)$.\n","2.  **Inner Derivative ($\\frac{dy}{dx}$):** The derivative of $x^2$ is $2x$.\n","3.  **Final Gradient:**\n","    $$\\frac{dz}{dx} = \\cos(y) \\cdot 2x = \\cos(x^2) \\cdot 2x$$\n","\n","\n","### **6. Example 3: Multivariate Function (Mean of Squares)**\n","\n","In this example, we calculate the gradient for a function that takes a vector input, squares each element, and finds the mean.\n","\n","**The Function:**\n","Given an input vector $x = [x_1, x_2, x_3]$:\n","$$Y = \\text{mean}(x^2) = \\frac{x_1^2 + x_2^2 + x_3^2}{3}$$\n","\n","**Gradient Calculation (Partial Derivatives):**\n","To find the gradients, we calculate the partial derivative of $Y$ with respect to each input component $x_i$:\n","\n","$$\n","\\frac{\\partial Y}{\\partial x_i} = \\frac{1}{3} \\cdot \\frac{d}{dx}(x_i^2) = \\frac{2x_i}{3}\n","$$\n","\n","**Numerical Example:**\n","If our input tensor is $x = [x_1, x_2, x_3]$, the gradient tensor stored in `x.grad` will be:\n","$$\n","\\text{gradients} = \\left[ \\frac{2x_1}{3}, \\frac{2x_2}{3}, \\frac{2x_3}{3} \\right]\n","$$\n","\n","* *Note:* This demonstrates that Autograd handles **vector-Jacobian products** automatically, computing derivatives for every element in the tensor simultaneously.\n","\n","\n","\n","\n","## 5Ô∏è‚É£ Computational Graph (Core Idea)\n","\n","During the **forward pass**:\n","\n","* Each operation becomes a **node**\n","* Tensors flow through operations\n","* Graph is dynamically built\n","\n","During the **backward pass**:\n","\n","* Gradients flow **backward**\n","* Each node applies local derivative\n","* Chain rule connects everything\n","\n","---\n","\n","## 6Ô∏è‚É£ Example: Single Neuron (Logistic Regression)\n","\n","### Forward Pass\n","\n","1. **Linear transformation**\n","\n","```text\n","z = w¬∑x + b\n","```\n","\n","2. **Activation (Sigmoid)**\n","\n","```text\n","≈∑ = œÉ(z) = 1 / (1 + e‚Åª·∂ª)\n","```\n","\n","3. **Loss (Binary Cross Entropy)**\n","\n","```text\n","L = -[y¬∑log(≈∑) + (1‚àíy)¬∑log(1‚àí≈∑)]\n","```\n","\n","---\n","\n","### Backward Pass (Gradients)\n","\n","Using chain rule:\n","\n","```text\n","‚àÇL/‚àÇw = (≈∑ ‚àí y) ¬∑ x\n","‚àÇL/‚àÇb = (≈∑ ‚àí y)\n","```\n","\n","Autograd computes these **without you writing the math**.\n","\n","---\n","\n","## 7Ô∏è‚É£ Neural Networks & Autograd\n","\n","In deep networks:\n","\n","* Each layer adds nodes to the graph\n","* Backward pass propagates gradients layer by layer\n","* Autograd handles:\n","\n","  * Weight gradients\n","  * Bias gradients\n","  * Intermediate tensor gradients\n","\n","---\n","\n","## 8Ô∏è‚É£ Vector Example (Multiple Inputs)\n","\n","Given:\n","\n","```text\n","x = [x‚ÇÅ, x‚ÇÇ, x‚ÇÉ]\n","y = mean(x¬≤) = (x‚ÇÅ¬≤ + x‚ÇÇ¬≤ + x‚ÇÉ¬≤) / 3\n","```\n","\n","Gradients:\n","\n","```text\n","‚àÇy/‚àÇx‚ÇÅ = 2x‚ÇÅ / 3\n","‚àÇy/‚àÇx‚ÇÇ = 2x‚ÇÇ / 3\n","‚àÇy/‚àÇx‚ÇÉ = 2x‚ÇÉ / 3\n","```\n","\n","Autograd computes each partial derivative automatically.\n","\n","---\n","\n","## 9Ô∏è‚É£ Key Autograd Properties\n","\n","* Gradients are stored in `.grad`\n","* Gradients accumulate by default\n","* Graph is freed after `.backward()` (unless retained)\n","* Works only for tensors with `requires_grad=True`\n","\n","---\n","\n","## üß† Mental Model (VERY IMPORTANT)\n","\n","> **Forward pass builds the graph**\n","> **Backward pass walks the graph in reverse using chain rule**\n","\n","You write:\n","\n","```python\n","loss.backward()\n","```\n","\n","PyTorch does:\n","\n","```text\n","Apply ‚àÇL/‚àÇoutput\n","‚Üí ‚àÇoutput/‚àÇhidden\n","‚Üí ‚àÇhidden/‚àÇweights\n","```\n","\n","---\n","\n","## üöÄ Why Autograd Is Powerful\n","\n","* Enables deep learning at scale\n","* Makes experimentation fast\n","* Eliminates manual derivative bugs\n","* Works for arbitrarily complex models\n","\n","---\n","\n","## ‚úÖ One-Line Summary\n","\n","> **Autograd = automatic gradient computation using dynamic computation graphs**\n"],"metadata":{"id":"012wiepTN5hP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXPFYGopf4KV","executionInfo":{"status":"ok","timestamp":1766644710198,"user_tz":-330,"elapsed":19,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"outputs":[],"source":["# Define a function that computes the derivative of y = x^2\n","def dy_dx(x):\n","    # The derivative of x^2 with respect to x is 2x\n","    return 2 * x\n","\n","# Call the function with x = 3\n","dy_dx(3)"]},{"cell_type":"markdown","source":["## üîÅ Manual Gradient Computation (dy/dx)\n","\n","This example demonstrates **manual differentiation**, which helps build intuition\n","for how **backpropagation** works.\n","\n","---\n","\n","## 1Ô∏è‚É£ The Function\n","\n","```text\n","y = x¬≤\n","````\n","\n","This is a simple quadratic function.\n","\n","---\n","\n","## 2Ô∏è‚É£ Derivative\n","\n","Using basic calculus:\n","\n","```text\n","dy/dx = 2x\n","```\n","\n","This tells us:\n","\n","* How fast `y` changes when `x` changes\n","* The **slope** of the function at any point `x`\n","\n","---\n","\n","## 3Ô∏è‚É£ Python Implementation\n","\n","```python\n","def dy_dx(x):\n","    return 2 * x\n","```\n","\n","Calling:\n","\n","```python\n","dy_dx(3)\n","```\n","\n","Gives:\n","\n","```text\n","6\n","```\n","\n","---\n","\n","## 4Ô∏è‚É£ Interpretation\n","\n","At `x = 3`:\n","\n","* The slope of `y = x¬≤` is `6`\n","* Increasing `x` slightly will increase `y` rapidly\n","\n","---\n","\n","## 5Ô∏è‚É£ Why This Matters in ML\n","\n","In machine learning:\n","\n","* `x` ‚Üí model parameters (weights)\n","* `y` ‚Üí loss\n","\n","Gradients tell us:\n","\n","```text\n","How should weights change to reduce loss?\n","```\n","\n"],"metadata":{"id":"AZbBnI4hO0uo"}},{"cell_type":"markdown","source":["---\n","\n","## 6Ô∏è‚É£ Connection to PyTorch Autograd\n","\n","Manual:\n","\n","```python\n","dy_dx(3)  # 6\n","```\n","\n","Autograd equivalent:"],"metadata":{"id":"17Tm-iTqPTG_"}},{"cell_type":"code","source":["import torch\n","import math"],"metadata":{"id":"RRT3DSJ_Tn4r","executionInfo":{"status":"ok","timestamp":1766645994002,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 1. Forward Pass Setup\n","# Create a tensor 'x' with value 3.0.\n","# requires_grad=True is the SWITCH that turns on Autograd.\n","# It tells PyTorch: \"Please track every operation on this variable so\n","# we can calculate derivatives later.\"\n","x = torch.tensor(3.0, requires_grad=True)"],"metadata":{"id":"bm_ra2CG21GV","executionInfo":{"status":"ok","timestamp":1766645014056,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 2. Define the Function (Computational Graph)\n","# Operation: y = x^2\n","# Forward Pass: 3^2 = 9\n","# PyTorch builds a \"graph\" in the background connecting x to y.\n","y = x**2\n","\n","print(\"x:\", x)  # Output: tensor(3., requires_grad=True)\n","print(\"y:\", y)  # Output: tensor(9., grad_fn=<PowBackward0>)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w93PDIuLPpKT","executionInfo":{"status":"ok","timestamp":1766645015264,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"ec08959d-86d1-4705-ff5d-1c09a3e557e3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["x: tensor(3., requires_grad=True)\n","y: tensor(9., grad_fn=<PowBackward0>)\n"]}]},{"cell_type":"code","source":["# 3. Backward Pass (Backpropagation)\n","# This command triggers the Chain Rule.\n","# It calculates dy/dx for every variable involved in creating 'y'.\n","# Mathematically: d(x^2)/dx = 2x.\n","y.backward()"],"metadata":{"id":"KvSsUKuePRsQ","executionInfo":{"status":"ok","timestamp":1766645017126,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 4. Check the Gradient\n","# Since x = 3, the gradient is 2 * 3 = 6.\n","# This value is stored in the .grad attribute of x.\n","x.grad  # Output: tensor(6.)"],"metadata":{"id":"nvMNlqM521TC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766645020795,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"5de1c0bf-1179-42d4-d8da-4942cdd7f593"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["> Autograd does **exactly what we did manually**, but for large computation graphs.\n","\n","---\n","\n","## üß† Mental Model\n","\n","> **Derivative = sensitivity**\n","\n","Gradients answer:\n","\n","* Which direction to move?\n","* How big should the step be?\n","\n","This is the foundation of **gradient descent**.\n","\n","---\n","\n","## ‚úÖ One-Line Summary\n","\n","> Manual differentiation builds intuition; Autograd scales it to neural networks.\n","\n","### Important Rules to Remember\n","‚úÖ Rule 1: Only leaf tensors store gradients\n","\n","* x is a leaf tensor\n","* y is not\n","\n","‚úÖ Rule 2: Gradients accumulate\n","\n","Calling .backward() again will add gradients unless reset:\n","\n","`x.grad.zero_()`\n","\n","‚úÖ Rule 3: .backward() works automatically\n","\n","You never manually write derivatives for complex graphs.\n","\n","> Forward pass builds the graph\n","> Backward pass walks the graph backward using chain rule\n","\n","```\n","x ‚Üí y\n","‚Üë\n","gradient flows backward\n","```\n","üöÄ Why This Matters in Deep Learning\n","\n","* x ‚Üí model parameters (weights)\n","* y ‚Üí loss\n","* x.grad ‚Üí how to update weights\n","\n","This is the foundation of:\n","* Gradient Descent\n","* Backpropagation\n","* Neural network training"],"metadata":{"id":"CgmurW8WQB3P"}},{"cell_type":"code","source":["# --- MANUAL CALCULATION ---\n","# y = x^2\n","# Function: z = sin(y) = sin(x^2)\n","# Chain Rule: dz/dx = dz/dy * dy/dx\n","#           = cos(y) * 2x\n","#           = cos(x^2) * 2x\n","def dz_dx(x):\n","    return 2 * x * math.cos(x**2)"],"metadata":{"id":"h6avVSli-nzp","executionInfo":{"status":"ok","timestamp":1766646012847,"user_tz":-330,"elapsed":43,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Calculate Manual Gradient for x = 4\n","# 2*4 * cos(16) ‚âà 8 * (-0.957) ‚âà -7.66\n","\n","print(f\"Manual Result: {dz_dx(4)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIiTKRH8TvVb","executionInfo":{"status":"ok","timestamp":1766646020355,"user_tz":-330,"elapsed":26,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"d0aa17b5-3883-400e-dc6b-cfdb1f033a7c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual Result: -7.661275842587077\n"]}]},{"cell_type":"code","source":["# --- PYTORCH AUTOGRAD ---\n","\n","# 1. Create a scalar tensor with gradient tracking enabled\n","x = torch.tensor(4.0, requires_grad=True)\n","\n","# 2. Forward Passes\n","# Forward pass: y = x^2\n","y = x ** 2          # y = 16\n","\n","# Forward pass: z = sin(y)\n","z = torch.sin(y)    # z = sin(16) ‚âà -0.2879"],"metadata":{"id":"93QF7kYQTx_f","executionInfo":{"status":"ok","timestamp":1766646441327,"user_tz":-330,"elapsed":28,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["print(\"\\n--- Tensor State ---\")\n","print(f\"x: {x}\")\n","print(f\"y: {y}\")\n","print(f\"z: {z}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbG2OtWvTzso","executionInfo":{"status":"ok","timestamp":1766646105826,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"5b56c250-61b2-4b91-b39d-d4951df343a8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Tensor State ---\n","x: 4.0\n","y: 16.0\n","z: -0.2879033088684082\n"]}]},{"cell_type":"code","source":["# 3. Backward Pass\n","# Compute dz/dx automatically using autograd\n","# This triggers the chain rule calculation.\n","z.backward()"],"metadata":{"id":"k679f44SUGuK","executionInfo":{"status":"ok","timestamp":1766646116059,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# 4. Check Gradients of z with respect to x\n","print(\"\\n--- Gradients ---\")\n","print(f\"x.grad (dz/dx): {x.grad}\")  # Matches Manual Result (-7.66)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFaQcjlAUIko","executionInfo":{"status":"ok","timestamp":1766646123696,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"e3350398-d13d-4ad6-b4d8-b97d677f2c72"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Gradients ---\n","x.grad (dz/dx): -7.661275863647461\n"]}]},{"cell_type":"code","source":["# CRITICAL NOTE:\n","# y.grad will be None by default because y is NOT a leaf tensor\n","# PyTorch automatically frees the gradients of \"intermediate\" nodes (like y)\n","# to save memory. It only keeps gradients for \"Leaf\" nodes (like x).\n","\n","print(f\"y.grad (dz/dy): {y.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLEKUxkaUKbz","executionInfo":{"status":"ok","timestamp":1766646131961,"user_tz":-330,"elapsed":621,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"30467b2c-39cb-42d0-8e5c-d1fce531fa48"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["y.grad (dz/dy): None\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3863262715.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n","  print(f\"y.grad (dz/dy): {y.grad}\")\n"]}]},{"cell_type":"markdown","source":["# üîó Chain Rule with PyTorch Autograd (Manual vs Automatic)\n","\n","This example demonstrates:\n","- Manual differentiation using calculus\n","- Automatic differentiation using PyTorch Autograd\n","- How the **chain rule** is applied internally\n","\n","---\n","\n","## 1Ô∏è‚É£ Mathematical Setup\n","\n","Define:\n","```text\n","y = x¬≤\n","z = sin(y) = sin(x¬≤)\n","````\n","\n","---\n","\n","## 2Ô∏è‚É£ Manual Derivative (Chain Rule)\n","\n","Using calculus:\n","\n","```text\n","dz/dx = dz/dy √ó dy/dx\n","```\n","\n","Where:\n","\n","```text\n","dz/dy = cos(y)\n","dy/dx = 2x\n","```\n","\n","So:\n","\n","```text\n","dz/dx = 2x ¬∑ cos(x¬≤)\n","```\n","\n","At `x = 4`:\n","\n","```text\n","dz/dx = 2 √ó 4 √ó cos(16)\n","```\n","\n","---\n","\n","## 3Ô∏è‚É£ PyTorch Autograd Computation\n","\n","```python\n","x = torch.tensor(4.0, requires_grad=True)\n","y = x ** 2\n","z = torch.sin(y)\n","z.backward()\n","```\n","\n","Autograd automatically:\n","\n","* Builds the computation graph\n","* Applies the chain rule\n","* Computes `dz/dx`\n","\n","---\n","\n","## 4Ô∏è‚É£ Accessing Gradients\n","\n","```python\n","x.grad\n","```\n","\n","This stores:\n","\n","```text\n","‚àÇz / ‚àÇx\n","```\n","\n","Matches the manual derivative result ‚úÖ\n","\n","---\n","\n","## 5Ô∏è‚É£ Why is `y.grad` None? ‚ö†Ô∏è (IMPORTANT)\n","\n","```python\n","y.grad  # None\n","```\n","\n","### Reason:\n","\n","* `y` is an **intermediate tensor**\n","* Only **leaf tensors** store gradients by default\n","\n","Leaf tensor:\n","\n","```python\n","x = torch.tensor(..., requires_grad=True)\n","```\n","\n","Non-leaf tensor:\n","\n","```python\n","y = x ** 2\n","```\n","\n","---\n","\n","## 6Ô∏è‚É£ How to Access Intermediate Gradients (Advanced)\n","\n","If you really need `y.grad`:\n","\n","```python\n","y.retain_grad()\n","z.backward()\n","y.grad\n","```\n","\n","‚ö†Ô∏è Use this only for:\n","\n","* Debugging\n","* Visualization\n","* Learning\n","\n","Not for normal training.\n","\n","---\n","\n","## üß† Mental Model\n","\n","> **Forward pass builds a computation graph**\n","> **Backward pass applies chain rule from output to inputs**\n","\n","```text\n","x ‚Üí (square) ‚Üí y ‚Üí (sin) ‚Üí z\n","‚Üë                    |\n","‚îî‚îÄ‚îÄ‚îÄ‚îÄ gradient flows ‚îÄ‚îò\n","```\n","\n","---\n","\n","## üöÄ Why This Matters in Deep Learning\n","\n","* Neural networks are just **very large chain-rule graphs**\n","* Autograd handles:\n","\n","  * Thousands of operations\n","  * Millions of parameters\n","* You only write:\n","\n","```python\n","loss.backward()\n","```\n","\n","---\n","\n","## ‚úÖ Key Takeaways\n","\n","‚úî Autograd matches manual calculus\n","‚úî Chain rule is applied automatically\n","‚úî Gradients stored only for leaf tensors\n","‚úî Intermediate gradients are optional\n","\n","---\n","\n","## üîë One-Line Summary\n","\n","> **Autograd is chain rule at scale, implemented automatically.**\n"],"metadata":{"id":"fF-PUHtKUjpq"}},{"cell_type":"code","source":["# 1. Inputs\n","\n","# Inputs (single data point)\n","x = torch.tensor(6.7)  # Input feature (scalar)\n","y = torch.tensor(0.0)  # True label (Target is 0; binary classification: 0 or 1)\n","\n","# Model parameters\n","w = torch.tensor(1.0)  # Weight\n","b = torch.tensor(0.0)  # Bias"],"metadata":{"id":"IDA7vJD6TERe","executionInfo":{"status":"ok","timestamp":1766646892242,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# 2. Define Loss Function (Binary Cross-Entropy Loss)\n","# Formula: L = -[y * log(p) + (1-y) * log(1-p)]\n","def binary_cross_entropy_loss(prediction, target):\n","    # Small value to avoid log(0), which is undefined\n","    epsilon = 1e-8\n","\n","    # Clamp restricts predictions to be strictly between [0.00000001, 0.99999999]\n","    # for numerical stability\n","    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n","\n","    # Binary Cross-Entropy formula\n","    return -(target * torch.log(prediction) +\n","             (1 - target) * torch.log(1 - prediction))\n",""],"metadata":{"id":"cUq5YU4WWQln","executionInfo":{"status":"ok","timestamp":1766646895879,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# 3. Forward Pass\n","\n","# Linear transformation\n","z = w * x + b          # Linear Step: 1.0 * 6.7 + 0 = 6.7\n","\n","# Apply sigmoid activation to get probability\n","# y_pred is in range (0, 1)\n","y_pred = torch.sigmoid(z) # Activation Step: 1 / (1 + e^-6.7) ‚âà 0.9988"],"metadata":{"id":"CNcSnxKFVw6_","executionInfo":{"status":"ok","timestamp":1766646923620,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# 4. Calculate Loss (Compute binary cross-entropy loss)\n","# The model is VERY confident (99.88%) that the class is 1.\n","# BUT the True Label is 0.\n","# This is a \"Wrong and Confident\" prediction, so the loss should be HIGH.\n","loss = binary_cross_entropy_loss(y_pred, y)"],"metadata":{"id":"Ysa6OOlAVzkI","executionInfo":{"status":"ok","timestamp":1766646964770,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["print(f\"Logit (z): {z:.4f}\")\n","print(f\"Prediction (y_pred): {y_pred:.4f}\")\n","print(f\"Loss: {loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVffyRjrWXHK","executionInfo":{"status":"ok","timestamp":1766646970600,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"a9e14534-0060-4399-85aa-9ebada6730e4"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit (z): 6.7000\n","Prediction (y_pred): 0.9988\n","Loss: 6.7012\n"]}]},{"cell_type":"markdown","source":["# üîê Logistic Regression Forward Pass + Binary Cross-Entropy Loss\n","\n","This example implements **binary classification** from scratch using:\n","- Linear model\n","- Sigmoid activation\n","- Binary Cross-Entropy (BCE) loss\n","\n","---\n","\n","## 1Ô∏è‚É£ Problem Setup\n","\n","We are solving a **binary classification** problem.\n","\n","- Input feature: `x = 6.7`\n","- True label: `y = 0` (negative class)\n","\n","---\n","\n","## 2Ô∏è‚É£ Model Parameters\n","\n","```text\n","z = w¬∑x + b\n","````\n","\n","Where:\n","\n","* `w` = weight\n","* `b` = bias\n","\n","This is a **linear transformation**.\n","\n","---\n","\n","## 3Ô∏è‚É£ Sigmoid Activation\n","\n","```python\n","y_pred = torch.sigmoid(z)\n","```\n","\n","### Formula:\n","\n","```text\n","œÉ(z) = 1 / (1 + e‚Åª·∂ª)\n","```\n","\n","### Output range:\n","\n","```text\n","0 < y_pred < 1\n","```\n","\n","Interpretation:\n","\n","* Output is a **probability**\n","* Used for binary classification\n","\n","---\n","\n","## 4Ô∏è‚É£ Binary Cross-Entropy Loss (BCE)\n","\n","```python\n","L = -[y¬∑log(yÃÇ) + (1 ‚àí y)¬∑log(1 ‚àí yÃÇ)]\n","```\n","\n","Where:\n","\n","* `y` = true label\n","* `yÃÇ` = predicted probability\n","\n","---\n","\n","### Special Cases\n","\n","| True Label `y` | Loss Simplifies To |\n","| -------------- | ------------------ |\n","| `y = 1`        | `-log(yÃÇ)`         |\n","| `y = 0`        | `-log(1 ‚àí yÃÇ)`     |\n","\n","---\n","\n","## 5Ô∏è‚É£ Why Clamp the Prediction? ‚ö†Ô∏è\n","\n","```python\n","prediction = torch.clamp(prediction, Œµ, 1 ‚àí Œµ)\n","```\n","\n","### Reason:\n","\n","* `log(0)` ‚Üí `‚àí‚àû`\n","* Causes numerical instability\n","\n","Clamping keeps:\n","\n","```text\n","Œµ < prediction < 1 ‚àí Œµ\n","```\n","\n","---\n","\n","## 6Ô∏è‚É£ Full Forward Pass Flow\n","\n","```text\n","x ‚Üí (w¬∑x + b) ‚Üí z\n","z ‚Üí sigmoid(z) ‚Üí yÃÇ\n","yÃÇ ‚Üí BCE(yÃÇ, y) ‚Üí loss\n","```\n","\n","This is the **entire forward pass** of logistic regression.\n","\n","---\n","\n","## 7Ô∏è‚É£ Connection to Deep Learning\n","\n","This exact pattern appears in:\n","\n","* Logistic Regression\n","* Final layer of binary classifiers\n","* Neural networks with sigmoid output\n","\n","In practice, PyTorch provides:\n","\n","```python\n","torch.nn.BCELoss()\n","torch.nn.BCEWithLogitsLoss()\n","```\n","\n","---\n","\n","## üß† Mental Model\n","\n","> **Linear model ‚Üí probability ‚Üí loss**\n","\n","Every deep learning classifier ultimately reduces to this idea.\n","\n","---\n","\n","## ‚úÖ One-Line Summary\n","\n","> Logistic regression = linear transformation + sigmoid + BCE loss\n"],"metadata":{"id":"7KZzsfELXZ2x"}},{"cell_type":"code","source":["# Derivatives (Manual Backprop)\n","\n","# Here we see what loss.backward() did using the CHAIN RULE.\n","# Chain Rule: dL/dw = (dL/dy_pred) * (dy_pred/dz) * (dz/dw)\n","\n","# 1. dL/d(y_pred)\n","# Derivative of Binary Cross-Entropy loss w.r.t. prediction\n","# BCE Loss Furomula: L = -(y*log(yÃÇ) + (1-y)*log(1-yÃÇ))\n","# The derivative is: dL/dyÃÇ = (yÃÇ - y) / (yÃÇ * (1 - yÃÇ))\n","dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))"],"metadata":{"id":"N31_2LUfV2cb","executionInfo":{"status":"ok","timestamp":1766647806210,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# 2. dy_pred/dz\n","# Derivative of Prediction (Sigmoid) w.r.t z ---\n","# sigmoid(z) = 1 / (1 + e^-z) or sigma(z) * (1 - sigma(z))\n","# d(sigmoid)/dz = yÃÇ * (1 - yÃÇ) or dy_pred_dz = y_pred * (1 - y_pred)\n","dy_pred_dz = y_pred * (1 - y_pred)"],"metadata":{"id":"2MJfcWduZoin","executionInfo":{"status":"ok","timestamp":1766647807475,"user_tz":-330,"elapsed":44,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# 3. Derivative of z w.r.t parameters (w and b) ---\n","# dz/dw and dz/db\n","# z = w*x + b\n","dz_dw = x        # derivative of z w.r.t. w\n","dz_db = 1        # derivative of z w.r.t. b"],"metadata":{"id":"sk51p_G0Zqff","executionInfo":{"status":"ok","timestamp":1766647854221,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# --- Final Calculation (Chain Rule) ---\n","# Multiply the links together to get the gradient for Weight (w)\n","\n","# dL/dw = dL/dyÃÇ * dyÃÇ/dz * dz/dw\n","dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n","\n","# dL/db = dL/dyÃÇ * dyÃÇ/dz * dz/db\n","dL_db = dloss_dy_pred * dy_pred_dz * dz_db"],"metadata":{"id":"OSP5rszqV5GG","executionInfo":{"status":"ok","timestamp":1766647856874,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n","print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAq3031TZz2f","executionInfo":{"status":"ok","timestamp":1766647859338,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"d27c2677-79cb-4774-964d-63872d39f9d0"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n","Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"]}]},{"cell_type":"code","source":["# OBSERVATION:\n","# Notice that 'dloss_dy_pred' and 'dy_pred_dz' cancel out parts of each other.\n","# (y_pred - y) / (outcome * (1-outcome)) * (outcome * (1-outcome))\n","# Simply becomes: (y_pred - y)\n","# So, dL/dw is usually simplified to just: (y_pred - y) * x"],"metadata":{"id":"ZMss2SE0a6le","executionInfo":{"status":"ok","timestamp":1766647901723,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["## Key Mathematical Concept Used: The Chain Rule\n","\n","The code manually calculates gradients by breaking the neural network into 3 stages. To find how the **Weight ($w$)** affects the **Loss ($L$)**, we multiply the derivatives of each stage:\n","\n","$$\\frac{\\partial L}{\\partial w} = \\underbrace{\\frac{\\partial L}{\\partial \\hat{y}}}_{\\text{Loss changes as pred changes}} \\cdot \\underbrace{\\frac{\\partial \\hat{y}}{\\partial z}}_{\\text{Pred changes as z changes}} \\cdot \\underbrace{\\frac{\\partial z}{\\partial w}}_{\\text{z changes as weight changes}}$$\n","\n","1. $\\frac{\\partial L}{\\partial \\hat{y}}$: Calculated as `dloss_dy_pred`.\n","\n","2. $\\frac{\\partial \\hat{y}}{\\partial z}$: Calculated as `dy_pred_dz` (Sigmoid derivative).\n","\n","3. $\\frac{\\partial z}{\\partial w}$: Calculated as `x` (Input value)."],"metadata":{"id":"R9Sl_ADCbNkp"}},{"cell_type":"code","source":["# Automation Verification (PyTorch)\n","\n","# 1. SETUP: DATA AND PARAMETERS\n","\n","# Input data (x) and Target label (y)\n","# These are fixed data points, so requires_grad is False by default.\n","\n","x = torch.tensor(6.7)\n","y = torch.tensor(0.0)"],"metadata":{"id":"3xdGCYz8V-Rh","executionInfo":{"status":"ok","timestamp":1766648218780,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Weights (w) and Bias (b)\n","# These are the parameters the model wants to learn (optimize).\n","# requires_grad=True tells PyTorch to track operations on these for 'autograd'.\n","\n","w = torch.tensor(1.0, requires_grad=True)\n","b = torch.tensor(0.0, requires_grad=True)"],"metadata":{"id":"9NTVaauoa1CZ","executionInfo":{"status":"ok","timestamp":1766648220175,"user_tz":-330,"elapsed":150,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["print(f\"Initial Weight: {w.item()}, Initial Bias: {b.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"096srlcbb52t","executionInfo":{"status":"ok","timestamp":1766648221605,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"f7a4267f-5c26-43cc-9e2a-2a545ee85a8f"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Weight: 1.0, Initial Bias: 0.0\n"]}]},{"cell_type":"code","source":["# 2. FORWARD PASS\n","\n","# Step A: Linear Combination (The \"Neuron\" math)\n","# z = w * x + b\n","z = w * x + b\n","\n","# Explanation: This calculates the weighted sum of inputs plus bias."],"metadata":{"id":"KTm9Pmf3a8cD","executionInfo":{"status":"ok","timestamp":1766648223379,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# Step B: Activation Function (Sigmoid)\n","# Squeezes the value of 'z' between 0 and 1 to make it a probability.\n","\n","y_pred = torch.sigmoid(z)"],"metadata":{"id":"Y2Z5ZqfxcLFg","executionInfo":{"status":"ok","timestamp":1766648231262,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# Step C: Calculate Loss (Binary Cross Entropy)\n","# This measures how wrong the prediction is compared to the target 'y'.\n","# Note: Usually we use torch.nn.functional.binary_cross_entropy\n","\n","loss = torch.nn.functional.binary_cross_entropy(y_pred, y)"],"metadata":{"id":"MzUFP5xqcNXN","executionInfo":{"status":"ok","timestamp":1766648240561,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["print(f\"Prediction (y_pred): {y_pred.item():.4f}\")\n","print(f\"Loss: {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02lAEKQgcR4y","executionInfo":{"status":"ok","timestamp":1766648259029,"user_tz":-330,"elapsed":39,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"1a5a076b-a6e5-47b6-cbb8-81702bd5c6f9"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction (y_pred): 0.9988\n","Loss: 6.7012\n"]}]},{"cell_type":"code","source":["# 3. AUTOMATIC BACKPROPAGATION (PYTORCH)\n","\n","# This single line triggers the \"Backward Pass\".\n","# PyTorch walks back through the computation graph to calculate gradients.\n","loss.backward()"],"metadata":{"id":"_y-YNk9DcTkl","executionInfo":{"status":"ok","timestamp":1766648270978,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["print(\"-\" * 30)\n","print(f\"Auto-calc Gradient w.r.t weight (w.grad): {w.grad.item():.4f}\")\n","print(f\"Auto-calc Gradient w.r.t bias (b.grad):   {b.grad.item():.4f}\")\n","print(\"-\" * 30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nA_AHRHcWXn","executionInfo":{"status":"ok","timestamp":1766648277465,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"453a44c7-1875-4206-d5fd-e3ea7c621fe2"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","Auto-calc Gradient w.r.t weight (w.grad): 6.6918\n","Auto-calc Gradient w.r.t bias (b.grad):   0.9988\n","------------------------------\n"]}]},{"cell_type":"code","source":["# 4. MANUAL BACKPROPAGATION (THE MATH)\n","# Here we manually recreate what loss.backward() did above using the CHAIN RULE.\n","# Chain Rule: dL/dw = (dL/dy_pred) * (dy_pred/dz) * (dz/dw)"],"metadata":{"id":"2IC_TeDJcZmu","executionInfo":{"status":"ok","timestamp":1766648294400,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# --- Link 1: Derivative of Loss w.r.t Prediction ---\n","# BCE Loss Formula: L = -[y*log(y_pred) + (1-y)*log(1-y_pred)]\n","# The derivative is: (y_pred - y) / (y_pred * (1 - y_pred))\n","dloss_dy_pred = (y_pred - y) / (y_pred * (1 - y_pred))"],"metadata":{"id":"NTdWYadJccCb","executionInfo":{"status":"ok","timestamp":1766648300547,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# --- Link 2: Derivative of Prediction (Sigmoid) w.r.t z ---\n","# Sigmoid derivative property: sigma(z) * (1 - sigma(z))\n","dy_pred_dz = y_pred * (1 - y_pred)"],"metadata":{"id":"9FCpWUFEcdkW","executionInfo":{"status":"ok","timestamp":1766648307181,"user_tz":-330,"elapsed":34,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# --- Link 3: Derivative of z w.r.t parameters (w and b) ---\n","# Since z = w*x + b:\n","dz_dw = x  # The derivative of (w*x) with respect to w is just x\n","dz_db = 1  # The derivative of (b) with respect to b is just 1"],"metadata":{"id":"XXSLwHstcfM7","executionInfo":{"status":"ok","timestamp":1766648313682,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# --- Final Calculation (Chain Rule) ---\n","# Multiply the links together to get the gradient for Weight (w)\n","dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw"],"metadata":{"id":"_JjsBKY4cgzP","executionInfo":{"status":"ok","timestamp":1766648320195,"user_tz":-330,"elapsed":29,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# Multiply the links together to get the gradient for Bias (b)\n","dL_db = dloss_dy_pred * dy_pred_dz * dz_db"],"metadata":{"id":"R3vxoc94cjxC","executionInfo":{"status":"ok","timestamp":1766648332196,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["print(f\"Manual Gradient w.r.t weight (dw):      {dL_dw.item():.4f}\")\n","print(f\"Manual Gradient w.r.t bias (db):        {dL_db.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcMGeUFfclrj","executionInfo":{"status":"ok","timestamp":1766648340150,"user_tz":-330,"elapsed":36,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"5c97c51c-159a-4638-a4bd-57a46c5b9194"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual Gradient w.r.t weight (dw):      6.6918\n","Manual Gradient w.r.t bias (db):        0.9988\n"]}]},{"cell_type":"code","source":["# OBSERVATION:\n","# Notice that 'dloss_dy_pred' and 'dy_pred_dz' cancel out parts of each other.\n","# (y_pred - y) / (outcome * (1-outcome)) * (outcome * (1-outcome))\n","# Simply becomes: (y_pred - y)\n","# So, dL/dw is usually simplified to just: (y_pred - y) * x"],"metadata":{"id":"SAkScQeicnQu","executionInfo":{"status":"ok","timestamp":1766648346515,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["# üîÅ Manual Backpropagation vs PyTorch Autograd (Binary Classification)\n","\n","This example shows:\n","- Manual gradient computation using calculus\n","- Verification using PyTorch Autograd\n","- How the **chain rule** powers backpropagation\n","\n","---\n","\n","## 1Ô∏è‚É£ Model Definition\n","\n","We use **logistic regression** for binary classification.\n","\n","### Forward equations:\n","\n","```text\n","z = w¬∑x + b\n","yÃÇ = sigmoid(z)\n","L = ‚àí[y¬∑log(yÃÇ) + (1‚àíy)¬∑log(1‚àíyÃÇ)]\n","````\n","\n","---\n","\n","## 2Ô∏è‚É£ Manual Gradient Derivation (Chain Rule)\n","\n","We apply:\n","\n","```text\n","dL/dw = dL/dyÃÇ √ó dyÃÇ/dz √ó dz/dw\n","dL/db = dL/dyÃÇ √ó dyÃÇ/dz √ó dz/db\n","```\n","\n","---\n","\n","### Step 1: Loss derivative\n","\n","```text\n","dL/dyÃÇ = (yÃÇ ‚àí y) / (yÃÇ(1 ‚àí yÃÇ))\n","```\n","\n","---\n","\n","### Step 2: Sigmoid derivative\n","\n","```text\n","dyÃÇ/dz = yÃÇ(1 ‚àí yÃÇ)\n","```\n","\n","---\n","\n","### Step 3: Linear derivatives\n","\n","```text\n","dz/dw = x\n","dz/db = 1\n","```\n","\n","---\n","\n","### Final gradients\n","\n","```text\n","dL/dw = (yÃÇ ‚àí y) ¬∑ x\n","dL/db = (yÃÇ ‚àí y)\n","```\n","\n","This is the **core gradient rule** of logistic regression.\n","\n","---\n","\n","## 3Ô∏è‚É£ PyTorch Autograd Verification\n","\n","```python\n","loss.backward()\n","```\n","\n","What Autograd does internally:\n","\n","* Builds a computation graph during the forward pass\n","* Applies the chain rule automatically\n","* Stores gradients in `.grad`\n","\n","```python\n","w.grad  ‚Üí dL/dw\n","b.grad  ‚Üí dL/db\n","```\n","\n","---\n","\n","## 4Ô∏è‚É£ Why This Is Important\n","\n","* This is **exactly how neural networks learn**\n","* Deep networks are just:\n","\n","  > logistic regression stacked many times\n","* Autograd saves you from writing this math manually\n","\n","---\n","\n","## üß† Mental Model\n","\n","> **Forward pass builds the graph**\n","> **Backward pass applies the chain rule backward**\n","\n","```text\n","x ‚Üí z ‚Üí yÃÇ ‚Üí L\n","‚Üë    ‚Üë    ‚Üë\n","|____|____|  gradients flow back\n","```\n","\n","---\n","\n","## 5Ô∏è‚É£ Practical Insight\n","\n","In real projects, you would NOT implement BCE manually.\n","\n","Instead, use:\n","\n","```python\n","torch.nn.BCEWithLogitsLoss()\n","```\n","\n","Why?\n","\n","* Numerically stable\n","* Combines sigmoid + BCE\n","* Faster and safer\n","\n","---\n","\n","## ‚úÖ One-Line Summary\n","\n","> Manual gradients build intuition; Autograd scales it to deep networks.\n"],"metadata":{"id":"8F571hfuc3S3"}},{"cell_type":"code","source":["# PART 1: Gradients with Vector Input\n","\n","# 1. Define a vector tensor with 3 elements\n","# requires_grad=True tells PyTorch to track every operation on this tensor.\n","x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)"],"metadata":{"id":"4K33x71Gc3Bp","executionInfo":{"status":"ok","timestamp":1766648595830,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# 2. Define the function: y = mean(x^2)\n","# Mathematical Formula: y = (x1^2 + x2^2 + x3^2) / 3\n","# We use .mean() to reduce the vector to a single scalar value.\n","# PyTorch's .backward() usually requires a scalar (single number) output.\n","y = (x**2).mean()"],"metadata":{"id":"Q44xZkeAdlrf","executionInfo":{"status":"ok","timestamp":1766648602227,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["# 3. Calculate Gradients of y w.r.t. x\n","# This computes dy/dx for every element in x.\n","# The derivative of (x^2)/3 is (2x)/3.\n","y.backward()"],"metadata":{"id":"u237bG6mdnE_","executionInfo":{"status":"ok","timestamp":1766648608064,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# 4. View the result (Gradient is computed for EACH element of x)\n","# For x=1.0: (2*1)/3 = 0.6667\n","# For x=2.0: (2*2)/3 = 1.3333\n","# For x=3.0: (2*3)/3 = 2.0000\n","print(f\"Gradients for vector x: {x.grad}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BH6lYoTqdpLh","executionInfo":{"status":"ok","timestamp":1766648616603,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"015e579f-b9c4-4c05-ed93-da29d3f032cb"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradients for vector x: tensor([0.6667, 1.3333, 2.0000])\n"]}]},{"cell_type":"code","source":["# PART 2: Clearing Gradients\n","\n","# NOTE: The line below creates a BRAND NEW tensor.\n","# The previous 'x' and its gradients are discarded/overwritten.\n","x = torch.tensor(2.0, requires_grad=True)"],"metadata":{"id":"IsNFiNVqdrac","executionInfo":{"status":"ok","timestamp":1766648627642,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# Define function: y = x^2\n","y = x ** 2"],"metadata":{"id":"x8khImBZdt3o","executionInfo":{"status":"ok","timestamp":1766648635791,"user_tz":-330,"elapsed":36,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# Calculate Gradient\n","# Derivative of x^2 is 2x.\n","y.backward()"],"metadata":{"id":"yuKGIgDThWq1","executionInfo":{"status":"ok","timestamp":1766648646484,"user_tz":-330,"elapsed":64,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["print(f\"Gradient before clearing (2*x): {x.grad}\") # Should be 2*2 = 4.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QD9Q1MNTd0b9","executionInfo":{"status":"ok","timestamp":1766648663282,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"8691ccf3-a463-49a3-d3e0-553b9d813080"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient before clearing (2*x): 4.0\n"]}]},{"cell_type":"code","source":["# 5. CLEARING GRADIENTS (Crucial Step!)\n","# In PyTorch, gradients \"accumulate\" (add up) by default if you call backward() twice.\n","# We must manually set them to zero before the next training step.\n","# The underscore (_) in zero_() means \"in-place operation\" (modifies x directly).\n","x.grad.zero_()\n","\n","print(f\"Gradient after clearing: {x.grad}\") # Should be 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhYoUNk8d3Fd","executionInfo":{"status":"ok","timestamp":1766648674031,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"f148c705-10cd-462a-f0ca-46f78717c2e1"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient after clearing: 0.0\n"]}]},{"cell_type":"markdown","source":["## Key Concepts in this Snippet\n","\n","1. **Scalar vs. Vector Output (mean()):**\n","\n","* `backward()` works best on a Scalar (a single number like Loss).\n","\n","* Since `x` was a vector `[1, 2, 3]`, `x**2` was also a vector.\n","\n","* We used `.mean()` to squash that vector into a single number so we could calculate the gradient easily.\n","\n","2. Why `zero_()`? **(Gradient Accumulation)**\n","\n","* PyTorch assumes you might want to sum gradients from multiple passes (e.g., in Recurrent Neural Networks).\n","\n","* Therefore, if you don't call `x.grad.zero_()`, the next time you call `backward()`, the new gradient will be added to the old 4.0, resulting in an incorrect value (e.g., 8.0).\n","\n","* Best Practice: Always zero out gradients at the start of a training loop step.\n","\n","###‚úÖ One-Line Summary\n","\n","> Gradients are element-wise, scale with reductions, and accumulate unless cleared.\n","\n","> Forward pass computes values\n","\n","> Backward pass accumulates gradients\n","\n","> We must reset gradients before the next step"],"metadata":{"id":"bnYuzW9ZfH3s"}},{"cell_type":"code","source":["# 1. BASELINE: NORMAL TRACKING\n","\n","x = torch.tensor(2.0, requires_grad=True)\n","# Create tensor with gradient tracking\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q79DJYW0g7az","executionInfo":{"status":"ok","timestamp":1766649681017,"user_tz":-330,"elapsed":37,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"e0ccabc7-7aab-454e-b648-dbc2fa7821a9"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2., requires_grad=True)"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["# Forward pass\n","y = x ** 2\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Y1kXmDnhtnm","executionInfo":{"status":"ok","timestamp":1766649717718,"user_tz":-330,"elapsed":44,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"6b60f1f6-cc0e-46c3-c8a7-f0d1e29b2837"},"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4., grad_fn=<PowBackward0>)"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["# Backward Pass\n","y.backward()\n","print(f\"Baseline Gradient: {x.grad}\") # Output: 4.0\n","# Gradient is stored\n","# x.grad, i.e.,  dy/dx = 2x = 4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqmIOhQLh2ZF","executionInfo":{"status":"ok","timestamp":1766649745022,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"14a27d81-f206-468a-a7ec-1cfec4f89351"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Gradient: 4.0\n"]}]},{"cell_type":"code","source":["# OPTION 1: requires_grad_(False)\n","# ==========================================\n","# This is an IN-PLACE operation. It effectively flips a switch on the tensor itself.\n","# Useful when you want to \"freeze\" a parameter permanently (e.g., a pretrained layer).\n","\n","x.requires_grad_(False) # Disable gradient tracking IN-PLACE\n","# x is now treated as a constant, not a variable to optimize.\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcQ8WldihGGH","executionInfo":{"status":"ok","timestamp":1766649806367,"user_tz":-330,"elapsed":31,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"cd0f3367-2d24-457b-8b60-4d33bb7206ca"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.)"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["# Forward pass again\n","y = x ** 2\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YshVWvkhKfc","executionInfo":{"status":"ok","timestamp":1766649828525,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"f4cf8dc6-ba45-4982-82a8-02bfab93727d"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.)"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["# Since x has no grad tracking, y also has no gradient function (grad_fn=None).\n","# Backward now FAILS because y has no grad_fn\n","# y.backward()  ‚ùå RuntimeError\n","\n","try:\n","    y.backward() # THIS WILL FAIL\n","except RuntimeError as e:\n","    print(f\"\\nOption 1 Error: {e}\")\n","    # Error: \"element 0 of tensors does not require grad and does not have a grad_fn\""],"metadata":{"id":"xmmh8FUoiRix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OPTION 2: .detach()\n","# ==========================================\n","# This creates a NEW tensor that shares the same data but is disconnected\n","# from the computational graph.\n","# Useful when you need the *value* of a tensor for plotting or metrics\n","# without affecting the gradients of the original variable.\n","\n","x = torch.tensor(2.0, requires_grad=True) # Create new tensor with gradient tracking\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvbl6WrxhMTN","executionInfo":{"status":"ok","timestamp":1766649879189,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"5be58273-8de6-455b-d69a-fd1e9a04c8c7"},"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2., requires_grad=True)"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["# Detach creates a NEW tensor sharing data\n","# but WITHOUT computation graph\n","# Therefore 'z' is a copy of 'x' that does not track gradients.\n","z = x.detach()\n","z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cmelXE7hTXo","executionInfo":{"status":"ok","timestamp":1766649916700,"user_tz":-330,"elapsed":46,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"96a06f26-3e96-4f9c-969e-2fe4cd1a240b"},"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.)"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["# Forward pass on original x (tracked)\n","y = x ** 2    # y depends on x (Tracked)\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfKGlW3MhVJt","executionInfo":{"status":"ok","timestamp":1766649934614,"user_tz":-330,"elapsed":29,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"ae47d485-f535-49b3-a53d-5e1d824dfd3a"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4., grad_fn=<PowBackward0>)"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["# Forward pass on detached tensor (not tracked)\n","y1 = z ** 2   # y1 depends on z (Not Tracked)\n","y1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3NRB4ZFirfN","executionInfo":{"status":"ok","timestamp":1766649945943,"user_tz":-330,"elapsed":18,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"19028c5a-bfb2-4d8b-9891-1183e9b3d76f"},"execution_count":104,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.)"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["# Backward on y works\n","# Backward on y1 FAILS (no graph)\n","# y1.backward() ‚ùå RuntimeError\n","y.backward()  # This works! gradients flow back to x.\n","\n","print(f\"\\nOption 2 Gradient (from y): {x.grad}\")\n","\n","try:\n","    y1.backward() # THIS WILL FAIL\n","except RuntimeError as e:\n","    print(f\"Option 2 Error (from y1): {e}\")\n","    # Error: y1 has no history because z broke the chain."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5RiHvzMhX6n","executionInfo":{"status":"ok","timestamp":1766649595328,"user_tz":-330,"elapsed":51,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"550670a4-6d16-4396-8c66-b9485ad94559"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Option 2 Gradient (from y): 4.0\n","Option 2 Error (from y1): element 0 of tensors does not require grad and does not have a grad_fn\n"]}]},{"cell_type":"code","source":["# OPTION 3: torch.no_grad() (Context Manager)\n","# ==========================================\n","# This is a temporary \"zone\" where tracking is turned off.\n","# Best Practice for: Inference / Validation loops.\n","\n","x = torch.tensor(2.0, requires_grad=True)"],"metadata":{"id":"EcqtO_qShbGd","executionInfo":{"status":"ok","timestamp":1766649611454,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    y = x ** 2\n","    # Inside here, y.requires_grad is False\n","    print(f\"\\nInside no_grad block, requires_grad: {y.requires_grad}\")\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6Mr5I1Khddk","executionInfo":{"status":"ok","timestamp":1766650008233,"user_tz":-330,"elapsed":55,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}},"outputId":"1577c86a-5792-4df8-c092-63b69d8e9466"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Inside no_grad block, requires_grad: False\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(4.)"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["# Outside, tracking resumes for NEW operations\n","y_outside = x ** 2\n","y_outside.backward()\n","\n","# Backward fails because graph was never created\n","# y.backward() ‚ùå RuntimeError"],"metadata":{"id":"JYuZAIC4hgYP","executionInfo":{"status":"ok","timestamp":1766649629441,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anuj Chaudhary","userId":"03934067282409377867"}}},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":["# üö´ Disabling Gradient Tracking in PyTorch\n","\n","PyTorch provides **three official ways** to disable gradient tracking.\n","Each has a **different purpose**.\n","\n","---\n","\n","## 1Ô∏è‚É£ Why Disable Gradients?\n","\n","You should disable gradients when:\n","- Doing **inference**\n","- Evaluating models\n","- Freezing layers\n","- Saving memory\n","- Improving speed\n","\n","---\n","\n","## 2Ô∏è‚É£ Option 1 ‚Äî `requires_grad_(False)`\n","\n","```python\n","x.requires_grad_(False)\n","````\n","\n","### What it does:\n","\n","* Disables gradient tracking **in-place**\n","* Tensor stops being a leaf for autograd\n","\n","### Use when:\n","\n","* Freezing model parameters\n","\n","### ‚ö†Ô∏è Warning:\n","\n","* Future operations on `x` are NOT tracked\n","* `.backward()` will fail\n","\n","---\n","\n","## 3Ô∏è‚É£ Option 2 ‚Äî `detach()`\n","\n","```python\n","z = x.detach()\n","```\n","\n","### What it does:\n","\n","* Creates a new tensor\n","* Shares the same data\n","* **No computation graph**\n","\n","### Use when:\n","\n","* You want a tensor value but NOT gradients\n","* Logging, metrics, auxiliary computations\n","\n","### Key rule:\n","\n","```text\n","detach() cuts the computation graph\n","```\n","\n","---\n","\n","## 4Ô∏è‚É£ Option 3 ‚Äî `torch.no_grad()`\n","\n","```python\n","with torch.no_grad():\n","    y = x ** 2\n","```\n","\n","### What it does:\n","\n","* Temporarily disables gradient tracking\n","* Most memory-efficient option\n","\n","### Use when:\n","\n","* Model inference\n","* Validation\n","* Prediction\n","\n","---\n","\n","## 5Ô∏è‚É£ Comparison Table\n","\n","| Method                  | Permanent? | New Tensor? | Typical Use    |\n","| ----------------------- | ---------- | ----------- | -------------- |\n","| `requires_grad_(False)` | Yes        | No          | Freeze weights |\n","| `detach()`              | No         | Yes         | Cut graph      |\n","| `torch.no_grad()`       | No         | No          | Inference      |\n","\n","---\n","\n","## 6Ô∏è‚É£ Why `.backward()` Fails After Disabling\n","\n","Backpropagation requires:\n","\n","* A computation graph\n","* Tensors with `grad_fn`\n","\n","If gradients are disabled:\n","\n","```text\n","No graph ‚Üí No backward pass\n","```\n","\n","---\n","\n","## üß† Mental Model\n","\n","> **Autograd only tracks what you tell it to track**\n","\n","* `requires_grad=True` ‚Üí track\n","* `detach()` ‚Üí cut\n","* `no_grad()` ‚Üí ignore\n","* `requires_grad_(False)` ‚Üí stop forever\n","\n","---\n","\n","## 7Ô∏è‚É£ Best Practices (REAL-WORLD)\n","\n","### Training loop\n","\n","```python\n","optimizer.zero_grad()\n","loss.backward()\n","optimizer.step()\n","```\n","\n","---\n","\n","### Validation / Inference\n","\n","```python\n","with torch.no_grad():\n","    outputs = model(inputs)\n","```\n","\n","---\n","\n","### Freezing layers\n","\n","```python\n","for param in model.parameters():\n","    param.requires_grad = False\n","```\n","\n","---\n","\n","## ‚úÖ One-Line Summary\n","\n","> Disable gradients intentionally ‚Äî wrong usage silently breaks learning.\n","\n","\n","\n"],"metadata":{"id":"0sCCNPedjIOz"}},{"cell_type":"markdown","source":["| Method | What it does | Best Use Case |\n","| :--- | :--- | :--- |\n","| `x.requires_grad_(False)` | **Permanent** change to the tensor (In-place). | **Freezing Weights**: Locking specific layers (e.g., ResNet backbone) during fine-tuning so they don't update. |\n","| `x.detach()` | Creates a **new** tensor disconnected from the graph. | **Plotting/Metrics**: Converting tensors to NumPy for Matplotlib, or feeding output to a non-differentiable function. |\n","| `torch.no_grad()` | **Temporary** mode (Context Manager) to stop tracking. | **Inference/Validation**: Running model predictions. Saves massive amounts of memory by not storing gradients. |"],"metadata":{"id":"kS7OuQXgjUTC"}}]}